{"cells":[{"cell_type":"code","source":["raw_data = dbutils.widgets.get('outputFromLookup')\nimport ast\nimport pandas as pd\n#Converting str to dictionary\ndic_data = ast.literal_eval(raw_data) \nvalueList = []\nfor i in range(0, len(dic_data['value']), 1):\n  valueList.append(dic_data['value'][i])\ndf_pd = pd.DataFrame(valueList)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Cleaning of Data\nnltk.download('stopwords')\nupdated_review = []\n\nfor i in range(0, len(df_pd), 1):\n    review = df_pd.iloc[i, 2]\n    review = re.sub('[^a-zA-Z]', ' ', review).lower().split()\n    ps = PorterStemmer()\n    temp = []\n    for word in review:\n        if word not in set(stopwords.words('english')):\n            temp.append(ps.stem(word))\n    review = ' '.join(temp)\n    updated_review.append(review)\n\n    # Creating bag of words model\ncv = CountVectorizer(max_features=1500)\nx = cv.fit_transform(updated_review).toarray()\ny_index = df_pd.columns.get_loc(\"Sentiment\")\ny = df_pd.iloc[:, y_index:(y_index+1)]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\n# Fit data into model\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\n\n# Predict data using model\ny_pred = classifier.predict(x_test)\n\n# Calculate accuracy of model\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Create Pickle File\nimport pickle\nlocalFileName = '/opt/DecisionClassificationNLP.pkl'\npickle.dump(classifier, open(localFileName, 'wb'))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Send Pickle file to Azure Lake Storage\nfrom azure.common.credentials import ServicePrincipalCredentials\nfrom azure.datalake.store import core, lib, multithread\ntoken = lib.auth(tenant_id = '524b0e96-35a3-46ef-ade3-a76c1936a890', client_secret = ']Ow/n_mKU/ODr9jpnOAJ3dd4kal1TAOA', client_id = '5218cf9a-9856-49e1-8e21-10cc9d019054')\nadlsFileSystemClient = core.AzureDLFileSystem(token, store_name='trainingdatalakestr')\nadlsFileSystemClient.put(localFileName, 'TwitterData/DecisionClassificationNLP.pkl')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Read test csv file into pandas dataFrame\nwith adlsFileSystemClient.open('TwitterData/test.csv', 'rb') as f:\n  df = pd.read_csv(f, encoding ='ISO-8859-1')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["updated_review = []\n\nfor i in range(0, len(df_pd), 1):\n    review = df_pd.iloc[i, 2]\n    review = re.sub('[^a-zA-Z]', ' ', review).lower().split()\n    ps = PorterStemmer()\n    temp = []\n    for word in review:\n        if word not in set(stopwords.words('english')):\n            temp.append(ps.stem(word))\n    review = ' '.join(temp)\n    updated_review.append(review)\n\n    # Creating bag of words model\ncv = CountVectorizer(max_features=1500)\nx_test = cv.fit_transform(updated_review).toarray()\ny_test_index = df_pd.columns.get_loc(\"Sentiment\")\ny_test = df_pd.iloc[:, y_test_index:(y_test_index+1)]"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import os\nif not os.path.exists('/opt/pkl_folder'):\n  os.mkdir('/opt/pkl_folder')\n  \nadlsFileSystemClient.get('TwitterData/DecisionClassificationNLP.pkl', '/opt/pkl_folder/DecisionClassificationNLP.pkl')\npkl_file = open('/opt/pkl_folder/DecisionClassificationNLP.pkl', 'rb')\npkl_model = pickle.load(pkl_file)\ny_pred = pkl_model.predict(x_test)\naccuracy_pkl_model = accuracy_score(y_test, y_pred)\nprint(\"Accuracy from pkl model\", accuracy_pkl_model)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["import pandas as pd\nx_test = pd.DataFrame(y_pred, columns=['x_test'])\ny_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\nresult_df = pd.concat([x_test, y_pred_df], axis=1)\nspark_result_df = spark.createDataFrame(result_df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Save dataframe to cosmosDB\nwriteConfig = {\n \"Endpoint\" : \"https://predictresultdb.documents.azure.com:443/\",\n \"Masterkey\" : \"RC9QHfB7DAbpKkXT8spsuYNnYFBW07mYlpYuZATc4lk9atAWTKpoz3Gzzhe3M6cbHhFjEmppEExqDeBmP8EeVA==\",\n \"Database\" : \"ToDoList\",\n \"Collection\" : \"TwitterHistoricalData\",\n \"Upsert\" : \"true\" \n}\n\nspark_result_df.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode('overwrite').options(**writeConfig).save()\n"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"TwitterModel","notebookId":4041913505607426},"nbformat":4,"nbformat_minor":0}
